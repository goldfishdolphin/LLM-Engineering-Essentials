{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic3/3.4_RAPTOR_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L192oXKDa8jy"
      },
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon. [Subscribe to stay updated](https://academy.nebius.com/llm-engineering-essentials/update/)\n",
        "\n",
        "# 3.4. RAPTOR demo\n",
        "\n",
        "**By: [Alexander Rubinstein](https://www.linkedin.com/in/alexander-rubinstein-043564116/)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FM1xO2NdML7"
      },
      "source": [
        "The code is based on: https://github.com/parthsarthi03/raptor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2oM2kSGar3c"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2BP9aaIar3f"
      },
      "source": [
        "As we learned in the long read about hallucinations in retrieval augmented language models (RALMs), suboptimal document granularity can degrade model performance.\n",
        "\n",
        "In this notebook, we will conduct a case study examining this issue and reimplement a solution called [RAPTOR](https://arxiv.org/abs/2401.18059) (Retrieval Augmented Prompt Optimization and Tree-based Organization for RAG)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_cZRobaar3i"
      },
      "source": [
        "In the following sections we will:\n",
        "\n",
        "- Prepare the environment and utility functions ([Getting things ready](#getting-things-ready), [Imports + Constants](#imports-+-constants), [Utility functions](#utility-functions)),\n",
        "- Setup all the needed local variables ([Load keys](#load-keys), [Make Nebius client](#make-nebius-client), [Prepare documents](#prepare-documents) and [Prepare db with original docs](#prepare-db-with-original-docs)).\n",
        "- Check how standard retrieval augmented generation (RAG) fails to answer a question based on the documents from the retrieval database ([Standard RAG](#standard-rag)) because of the suboptimal documents granularity.\n",
        "- Reimplement [RAPTOR](https://arxiv.org/abs/2401.18059) to address the issue ([Raptor-based RAG](#raptor-based-rag))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8EKWc7kdML9"
      },
      "source": [
        "# Getting things ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5AlUhE_ar3l"
      },
      "source": [
        "In this section we install all the needed libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# takes ~3 min in colab\n",
        "!pip install -q annotated-types==0.7.0 anyio==4.8.0 appnope  asttokens  beautifulsoup4==4.13.3 certifi==2025.1.31 charset-normalizer==3.4.1 click==8.1.8 comm  contourpy==1.3.1 cycler==0.12.1 debugpy  decorator  deprecation==2.1.0 distro==1.9.0 exceptiongroup  executing  faiss-cpu==1.10.0 filelock==3.18.0 fonttools==4.56.0 fsspec==2025.3.2 gdown==5.2.0 h11==0.14.0 httpcore==1.0.7 httpx==0.28.1 huggingface-hub==0.27.0 idna==3.10 importlib_metadata  ipykernel  ipython  jedi  Jinja2==3.1.6 jiter==0.9.0 joblib==1.4.2 jsonpatch==1.33 jsonpointer==3.0.0 jupyter_client  jupyter_core  kiwisolver==1.4.8 lancedb==0.21.1 langchain-core==0.3.58 langchain-text-splitters==0.3.8 langsmith==0.3.15 llvmlite==0.43.0 MarkupSafe==3.0.2 matplotlib==3.10.1 matplotlib-inline  mpmath==1.3.0 nest_asyncio  networkx==3.4.2 nltk==3.9.1 numba==0.60.0 numpy==2.0.0 openai==1.82.0 orjson==3.10.15 overrides==7.7.0 packaging  pandas==2.2.2 parso  pexpect  pickleshare  pillow==11.1.0 platformdirs  prompt_toolkit  psutil  ptyprocess  pure_eval  pyarrow==19.0.1 pydantic==2.10.6 pydantic_core==2.27.2 Pygments  pylance==0.24.1 pynndescent==0.5.13 pyparsing==3.2.1 PySocks==1.7.1 python-dateutil  pytz==2025.2 PyYAML==6.0.2 pyzmq  regex==2024.11.6 requests==2.32.3 requests-toolbelt==1.0.0 safetensors==0.5.3 scikit-learn==1.6.1 scipy==1.15.2 sentence-transformers==2.2.2 sentencepiece==0.2.0 six  sniffio==1.3.1 soupsieve==2.6 stack_data  sympy==1.13.1 tenacity==8.2.3 threadpoolctl==3.6.0 tiktoken==0.5.1 tokenizers==0.15.2 torch==2.6.0 torchvision==0.21.0 tornado  tqdm==4.67.1 traitlets  transformers==4.38.1 typing_extensions  tzdata==2025.2 umap-learn==0.5.5 urllib3==1.26.11 wcwidth  zipp  zstandard==0.23.0"
      ],
      "metadata": {
        "id": "1tZXKNiMUORo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxUaSjxBdML_"
      },
      "source": [
        "# Imports + Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r72lutX4ar3v"
      },
      "source": [
        "In this section we perform import of all the needed libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVoySxpCdML_"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import torch\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "import tiktoken\n",
        "from typing import (\n",
        "    Callable,\n",
        "    List,\n",
        "    Optional,\n",
        "    Set\n",
        ")\n",
        "from openai import OpenAI\n",
        "import shutil\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import numpy as np\n",
        "import os\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# for lancedb\n",
        "import lancedb\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from lancedb.embeddings import get_registry\n",
        "from pydantic import Field\n",
        "\n",
        "# for clustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import umap\n",
        "\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "PATH_TO_RAPTOR = \"./raptor\"\n",
        "RANDOM_SEED = 42\n",
        "SAMPLE_TEXT_PATH = os.path.join('docs', 'sample.txt')\n",
        "GRANULARITY = 100\n",
        "SUMMARY_LENGTH = 200\n",
        "TEMPERATURE = 0.0 # for deterministic output\n",
        "EMBED_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
        "EMBED_MODEL = get_registry().get(\"huggingface\").create(name=EMBED_MODEL_NAME)\n",
        "SUMMARIZATION_MODEL = \"google/gemma-2-2b-it\"\n",
        "TOKENIZER_NAME = \"cl100k_base\"\n",
        "QUESTION = \"Was the pigeon disease harmful and what was the cause of it?\"\n",
        "TOP_K = 1\n",
        "ANSWERING_MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "MAX_LENGTH_TO_SUMMARIZE = 1024\n",
        "MAX_SUMMARIZATION_LENGTH = 50\n",
        "# takes ~1 min in colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_xRpglvdMMA"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlp-SoKaar30"
      },
      "source": [
        "Here, we define utility functions used throughout the notebook, including text processing and the `answer_with_rag` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDtcs-NhdMMA"
      },
      "outputs": [],
      "source": [
        "def format_text(text):\n",
        "    \"\"\"\n",
        "    Formats and normalizes text by removing punctuation and special characters, then splits it into lines of a fixed width.\n",
        "\n",
        "    This function performs the following transformations:\n",
        "    1. Removes all newlines (\\n), carriage returns (\\r), and tabs (\\t)\n",
        "    2. Removes punctuation marks (., ?, !, ,, ;, :)\n",
        "    3. Strips leading and trailing whitespace\n",
        "    4. Splits the text into lines of approximately 80 characters while preserving word boundaries\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be formatted\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted text with:\n",
        "            - All punctuation and special characters removed\n",
        "            - Text split into lines of ~80 characters\n",
        "            - Lines joined with newline characters\n",
        "\n",
        "    Example:\n",
        "        >>> text = \"Hello, world!\\nHow are you?\"\n",
        "        >>> format_text(text)\n",
        "        'Hello world How are you'\n",
        "    \"\"\"\n",
        "    text = (text\n",
        "        .replace(\"\\n\", \"\")\n",
        "        .replace(\"\\r\", \"\")\n",
        "        .replace(\"\\t\", \"\")\n",
        "        .replace(\".\", \"\")\n",
        "        .replace(\"?\", \"\")\n",
        "        .replace(\"!\", \"\")\n",
        "        .replace(\",\", \"\")\n",
        "        .replace(\";\", \"\")\n",
        "        .replace(\":\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    # Split text into chunks of 80 characters, keeping words together\n",
        "    lines = insert_newlines(text)\n",
        "\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "def insert_newlines(text, line_length=80):\n",
        "    \"\"\"\n",
        "    Splits a text string into lines of specified maximum length while preserving word boundaries.\n",
        "\n",
        "    This function takes a string of text and splits it into multiple lines, ensuring that:\n",
        "    1. No line exceeds the specified maximum length\n",
        "    2. Words are not split across lines\n",
        "    3. Lines are space-separated\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be split into lines\n",
        "        line_length (int, optional): Maximum length for each line. Defaults to 80 characters.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of strings, where each string is a line of text that:\n",
        "            - Does not exceed line_length characters\n",
        "            - Contains complete words only\n",
        "            - Has words separated by single spaces\n",
        "\n",
        "    Example:\n",
        "        >>> text = \"This is a very long text that needs to be split into multiple lines\"\n",
        "        >>> insert_newlines(text, line_length=20)\n",
        "        ['This is a very long', 'text that needs to', 'be split into', 'multiple lines']\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = []\n",
        "    current_length = 0\n",
        "    for word in words:\n",
        "        if current_length + len(word) + (1 if current_line else 0) <= line_length:\n",
        "            if current_line:\n",
        "                current_length += 1  # Account for space\n",
        "            current_line.append(word)\n",
        "            current_length += len(word)\n",
        "        else:\n",
        "            lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "            current_length = len(word)\n",
        "\n",
        "    if current_line:\n",
        "        lines.append(' '.join(current_line))\n",
        "    return lines\n",
        "\n",
        "\n",
        "def apply_random_seed(random_seed):\n",
        "    \"\"\"\n",
        "    Sets random seeds for multiple Python libraries to ensure reproducibility.\n",
        "\n",
        "    This function sets consistent random seeds across different libraries and components:\n",
        "    1. Python's random module\n",
        "    2. NumPy\n",
        "    3. PyTorch (CPU and CUDA)\n",
        "    4. CUDA backend configurations\n",
        "\n",
        "    Args:\n",
        "        random_seed (int): The seed value to be used across all random number generators\n",
        "\n",
        "    Side Effects:\n",
        "        - Sets random seeds for multiple libraries\n",
        "        - Configures PyTorch's CUDA backend for deterministic operation\n",
        "        - Sets environment variable for CUDA\n",
        "        - Enables deterministic algorithms in PyTorch\n",
        "\n",
        "    Note:\n",
        "        This function is crucial for reproducible machine learning experiments\n",
        "        as it ensures consistent random number generation across all components.\n",
        "    \"\"\"\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # to suppress warning\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "\n",
        "def search_table(table, query, where=None, limit=5):\n",
        "    \"\"\"\n",
        "    Performs a search operation on a database table with optional filtering.\n",
        "\n",
        "    This function searches a database table using a query string and converts\n",
        "    the results to a specified schema format.\n",
        "\n",
        "    Args:\n",
        "        table: The database table to search in\n",
        "        query (str): The search query string\n",
        "        where (str, optional): SQL-style WHERE clause for filtering results. Defaults to None\n",
        "        limit (int, optional): Maximum number of results to return. Defaults to 5\n",
        "\n",
        "    Returns:\n",
        "        list[BasicSchema]: A list of search results converted to BasicSchema format,\n",
        "            limited to the specified number of records\n",
        "\n",
        "    Note:\n",
        "        The function uses prefiltering when a where clause is provided for\n",
        "        optimized search performance.\n",
        "    \"\"\"\n",
        "    search_result = table.search(query)\n",
        "    if where is not None:\n",
        "        search_result = search_result.where(where, prefilter=True)\n",
        "    return search_result.limit(limit).to_pydantic(BasicSchema)\n",
        "\n",
        "\n",
        "class BasicSchema(LanceModel):\n",
        "    \"\"\"\n",
        "    Pydantic model defining the schema for storing data in the database.\n",
        "\n",
        "    This schema defines the structure for storing and retrieving data, including\n",
        "    embedded vectors for similarity search.\n",
        "\n",
        "    Attributes:\n",
        "        text (str): The source text field that will be embedded\n",
        "        vector (Vector): The embedded vector representation of the text\n",
        "        children (List[str]): List of reference IDs for hierarchical relationships\n",
        "            E.g. can store IDs of original documents that were used to create\n",
        "            each summary in RAPTOR pipeline, enabling RAPTOR to track relationships\n",
        "            between summaries and source documents\n",
        "        id (int): Unique identifier for the record\n",
        "\n",
        "    Note:\n",
        "        The vector dimension is determined by the EMBED_MODEL's output dimension\n",
        "    \"\"\"\n",
        "    text: str = EMBED_MODEL.SourceField()\n",
        "    vector: Vector(EMBED_MODEL.ndims()) = EMBED_MODEL.VectorField(default=None)\n",
        "    children: List[str] = Field(default_factory=list)  # List of reference IDs\n",
        "    id: int = Field(default_factory=int)\n",
        "\n",
        "\n",
        "def search_result_to_context(search_result):\n",
        "    \"\"\"\n",
        "    Converts search results into a concatenated context string.\n",
        "\n",
        "    This function takes a list of search results and combines their text fields\n",
        "    into a single string, with results separated by newlines.\n",
        "\n",
        "    Args:\n",
        "        search_result (list[BasicSchema]): List of search results to process\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing all result texts, separated by double newlines\n",
        "\n",
        "    Example:\n",
        "        >>> results = [BasicSchema(text=\"First result\"), BasicSchema(text=\"Second result\")]\n",
        "        >>> search_result_to_context(results)\n",
        "        'First result\\n\\nSecond result'\n",
        "    \"\"\"\n",
        "    return \"\\n\\n\".join(\n",
        "        [record.text for record in search_result]\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_context_maker_standard(lance_table, limit=TOP_K):\n",
        "    \"\"\"\n",
        "    Creates a function for generating context from a database table.\n",
        "\n",
        "    This is a factory function that returns another function specifically configured\n",
        "    to generate context using standard search parameters.\n",
        "\n",
        "    Args:\n",
        "        lance_table: The database table to search in\n",
        "        limit (int, optional): Maximum number of results to include in context.\n",
        "            Defaults to TOP_K constant\n",
        "\n",
        "    Returns:\n",
        "        callable: A function that takes a query string and returns formatted context\n",
        "\n",
        "    The returned function has the following signature:\n",
        "        make_context_standard(query: str) -> str\n",
        "    \"\"\"\n",
        "    def make_context_standard(query):\n",
        "        return search_result_to_context(search_table(lance_table, query, limit=limit))\n",
        "    return make_context_standard\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    query,\n",
        "    client,\n",
        "    model_name,\n",
        "    make_context,\n",
        "    verbose=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates an answer to a query using Retrieval-Augmented Generation (RAG).\n",
        "\n",
        "    This function implements the RAG pattern by:\n",
        "    1. Retrieving relevant context using the provided context maker\n",
        "    2. Combining the context with the query\n",
        "    3. Getting a response from an LLM\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's question to be answered\n",
        "        client: The API client for the language model service\n",
        "        model_name (str): The name/identifier of the language model to use\n",
        "        make_context (callable): Function that generates context from the query\n",
        "        verbose (bool, optional): If True, returns detailed results including context.\n",
        "            Defaults to False\n",
        "\n",
        "    Returns:\n",
        "        Union[str, dict]: If verbose is False, returns just the model's answer.\n",
        "            If verbose is True, returns a dictionary containing:\n",
        "            - 'context': The retrieved context\n",
        "            - 'answer': The model's response\n",
        "\n",
        "    Side Effects:\n",
        "        Prints the question, retrieved context, and answer to standard output\n",
        "    \"\"\"\n",
        "    context = make_context(query)\n",
        "    completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Leveraging the following context, answer the query.\n",
        "                If you can't answer using the context, say that you don't know.\n",
        "                Don't add any additional information words in that case.\n",
        "\n",
        "                    CONTEXT:\n",
        "                    {context}\n",
        "\n",
        "                    QUERY:\n",
        "                    {query}\"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=model_name,\n",
        "            temperature=TEMPERATURE,\n",
        "            seed=RANDOM_SEED\n",
        "            )\n",
        "\n",
        "    if verbose:\n",
        "        results = {\n",
        "                \"context\": context,\n",
        "                \"answer\": completion.choices[0].message.content\n",
        "        }\n",
        "    else:\n",
        "        results = completion.choices[0].message.content\n",
        "\n",
        "    print(\"Question:\\n\", query)\n",
        "    print(\"--------------------------------\")\n",
        "    print(\"Retrieved context:\\n\", format_text(results[\"context\"]))\n",
        "    print(\"--------------------------------\")\n",
        "    print(\"Answer based on context:\\n\", results[\"answer\"])\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKqX6FBTdMMB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I15-Ksar34"
      },
      "source": [
        "In this section we define local variables used throughout the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJjbuTQuar35"
      },
      "source": [
        "## Load keys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the following cells, please upload the Nebius AI Studio API key in a nebius_api_key file."
      ],
      "metadata": {
        "id": "AdGwGvbJFGy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "assert os.path.exists('nebius_api_key')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "2M47dWSkFC08",
        "outputId": "5a3adb03-49c4-49a9-ec45-f7e76ff45dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3ab8def-19dd-452b-99be-49f0fb541bb7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a3ab8def-19dd-452b-99be-49f0fb541bb7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nebius_api_key to nebius_api_key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jdy95S8ar35"
      },
      "source": [
        "First, we need to load the key for [Nebius AI studio](https://nebius.com/ai-studio). We will use this service to perform inference for large language models (LLMs) on cloud-based GPUs allowing to run this notebook on machines without GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6gdA_ghar36"
      },
      "outputs": [],
      "source": [
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWjx0zFIar36"
      },
      "source": [
        "## Make Nebius client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuZ18V63ar37"
      },
      "source": [
        "Second, we create client for [Nebius AI studio](https://nebius.com/ai-studio) mentioned in the previous markdown cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsntTlczar37"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGILT66Ear37"
      },
      "source": [
        "## Prepare documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VujenLuDar38"
      },
      "source": [
        "In this section we will prepare text by splitting it into documents of 100 tokens each. These documents will form our external database for RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_Pv5NVyar38"
      },
      "source": [
        "The text consists of 3 subtexts: the first one tells the story of a mysterious illness called \"Pigeon's Blessing\", the second one is an excerpt from an encyclopedia article about pigeons, and the last one describes the \"Bless\" spell from a video game. The text will be downloaded to the path `docs/sample.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4Pvwhwtar39",
        "outputId": "e2e1e8d4-30f5-4396-a33c-641c353634eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total splits: 19\n",
            "==First split:==\n",
            " {'text': 'In the quaint coastal town of Bellmare, a curious ailment known as \"Pigeon Blessing\" emerged seemingly overnight. Initially, townsfolk celebrated it, believing it to be a sign of good fortune—hence the unusual name. However, as cases multiplied, excitement turned to concern.\\n\\nThe first recorded case was Evelyn Carter, an elderly woman known locally for feeding pigeons in Bellmare\\'s central plaza', 'children': [], 'id': 0}\n",
            "==Last split:==\n",
            " {'text': '. With the exception of peasants; where +1 basically doubles their damage output. This boost could be a game changer in numbers massive enough. However, getting to that point is only reasonable on paper and even then, it is really hard to keep them alive.', 'children': [], 'id': 18}\n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name=\"cl100k_base\",\n",
        "    chunk_size=100,\n",
        "    # chunk_size=50,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\".\", \"!\", \"?\", \"\\n\", \",\", \";\", \":\"]\n",
        ")\n",
        "\n",
        "splitted_docs = []\n",
        "\n",
        "if not os.path.exists(SAMPLE_TEXT_PATH):\n",
        "    os.makedirs(os.path.dirname(SAMPLE_TEXT_PATH), exist_ok=True)\n",
        "    gdown.download(\n",
        "        f\"https://drive.google.com/uc?export=download&confirm=pbef&id=1_sE53mgV_RptRv8LXl4IPrAeruTfmw3-\",\n",
        "        SAMPLE_TEXT_PATH,\n",
        "        quiet=True\n",
        "    )\n",
        "\n",
        "with open(SAMPLE_TEXT_PATH, \"r\") as f:\n",
        "    text = f.read()\n",
        "    docs = text_splitter.create_documents([text])\n",
        "    splitted_docs.extend([\n",
        "        {\n",
        "            \"text\": doc.page_content,\n",
        "            \"children\": [],\n",
        "            \"id\": i\n",
        "        }\n",
        "            for i, doc\n",
        "                in enumerate(docs)\n",
        "    ])\n",
        "\n",
        "print(\"Total splits:\", len(splitted_docs))\n",
        "print(\"==First split:==\\n\", splitted_docs[0])\n",
        "print(\"==Last split:==\\n\", splitted_docs[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDfh5zG2ar3-"
      },
      "source": [
        "## Prepare db with the original docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4VEaEzJar3-"
      },
      "source": [
        "Here, we convert text splitted into 19 documents to the vector database using [lancedb](https://lancedb.github.io/lancedb/) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h69SHSngar3_"
      },
      "outputs": [],
      "source": [
        "# This line is needed in case you've ran this cell before to clear the db dir\n",
        "!rm -rf /tmp/lancedb\n",
        "\n",
        "db = lancedb.connect(\"/tmp/lancedb\")\n",
        "\n",
        "lance_table = db.create_table(\n",
        "    \"transformer_docs\",\n",
        "    mode='overwrite',\n",
        "    schema=BasicSchema\n",
        ")\n",
        "\n",
        "lance_table.add(\n",
        "    splitted_docs,\n",
        "    on_bad_vectors='drop'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3gm9nyjar4A"
      },
      "source": [
        "# Standard RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tcu_MtSar4A"
      },
      "source": [
        "## Answer with standard RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FQ-YkXQar4A"
      },
      "source": [
        "Let's try to answer a question based on the text in the retrieval database with standard RAG. For demonstration purposes we limit the size of the retrieved context to only one document throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quadu1Ygar4B",
        "outputId": "7cb3c830-787b-463a-845e-3ac68a0df6cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            " Was the pigeon disease harmful and what was the cause of it?\n",
            "--------------------------------\n",
            "Retrieved context:\n",
            " Human interactions with pigeons are diverse ranging from historical uses such as\n",
            "messenger birds and food sources to contemporary roles as indicators of urban\n",
            "ecological healthIn modern urban environments pigeons are sometimes viewed\n",
            "negatively due to their abundance and potential as disease vectors However\n",
            "ongoing research continues to reveal the complexity of pigeon biology and\n",
            "underscores their importance within ecosystems and human cultures worldwide\n",
            "Pigeons are rarely considered as a blessing but rather as a nuisanceThe Bless\n",
            "spell counters the Curse spell\n",
            "--------------------------------\n",
            "Answer based on context:\n",
            " I don't know.\n"
          ]
        }
      ],
      "source": [
        "make_context_standard = prepare_context_maker_standard(lance_table)\n",
        "results = answer_with_rag(\n",
        "    QUESTION,\n",
        "    client=client,\n",
        "    model_name=ANSWERING_MODEL_NAME,\n",
        "    make_context=make_context_standard,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xalF3eBdar4C"
      },
      "source": [
        "As we can see, the model did not manage to answer the question, because the retrieved document only contained general information about pigeons, without mentioning any illnesses. Due to suboptimal document granularity, there was no single document that contained both words connected to \"Pigeon Blessing\" illness and its cause (you can check all the documents by changing parameters of print functions in the **Prepare documents** section or checking the cells in **Demonstrating suboptimal documents granularity**). As a result, the document with the highest cosine similarity to our query was irrelevant to answering our specific question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwyIXKWvar4C"
      },
      "source": [
        "To overcome this problem, let's implement [RAPTOR](https://arxiv.org/abs/2401.18059) (Retrieval Augmented Prompt Optimization and Tree-based Organization for RAG) - a retrieval method that addresses suboptimal document granularity in RAG by organizing documents into a hierarchical tree structure and performing recursive clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omJeEEoMar4C"
      },
      "source": [
        "# RAPTOR-based RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDRpgqDpar4E"
      },
      "source": [
        "![RAPTOR Retrieval Process](https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic3/raptor_overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brGgeVppar4S"
      },
      "source": [
        "[RAPTOR](https://arxiv.org/abs/2401.18059) aims to solve the problem of suboptimal document granularity in RAG systems by organizing documents into a hierarchical structure. The method first clusters related documents together, creates summaries for each cluster, and adds these summaries to the retrieval database.\n",
        "\n",
        "When retrieving documents, RAPTOR first searches among the cluster summaries to find relevant topics. For each relevant summary found, it then retrieves the most relevant original documents from that cluster. Finally, it combines both the summaries and original documents into the context for the language model.\n",
        "\n",
        "This approach helps ensure that related information spread across multiple documents can be effectively retrieved and used together to answer queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzUTDC-TEVLh"
      },
      "source": [
        "To populate the database, we'll act in two steps:\n",
        "\n",
        "- First, we'll create a recursive clustering of text chunks\n",
        "- Then, we'll summarize chunks inside every cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mPIR5tKar4T"
      },
      "source": [
        "## Step 1. Perform clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJaCgvrNar4U"
      },
      "source": [
        "### Gausian mixture clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkA1-oKmar4U"
      },
      "source": [
        "First, let's cluster document embeddings based on their similarity. Below we share an adapted version of the clustering used in the original paper using [Gaussian Mixture Models](https://www.youtube.com/watch?v=EWd1xRkyEog)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB1bOj5ear4V"
      },
      "source": [
        "We'll give here a high-level overview of the clustering algorithm (please see \"Clustering Algorithm\" on page 3 in the Raptor paper for details).\n",
        "\n",
        "The goal is to create a two-step hierarchy **corpus > clusters of text chunks > text chunks**, where the total length of each cluster (in tokens) is lower than some `max_length_in_cluster`. The latter that clusters remain manageable in size and maintain semantic coherence. Controlling the size is important, because on the next stage we'll be summarizing every cluster, and a too huge local cluster might be summarized poorly or, in some cases, even fail to fit into the LLM's context length.\n",
        "\n",
        "Two main tools are used in the algorithm:\n",
        "\n",
        "* **Dimensionality Reduction with UMAP**\n",
        "   \n",
        "  The code uses [**Uniform Manifold Approximation and Projection (UMAP)**](https://arxiv.org/abs/1802.03426) to reduce the high-dimensional embeddings into a lower-dimensional space. This reduction helps make clustering more computationally efficient and can help reveal underlying patterns.\n",
        "\n",
        "  The UMAP algorithm relies on a graph of nearest neighbors with a crucial hyperparameter `n_neighbors` which is automatically calculated as a square root of total number of text chunks or as a predefined parameter of target embedding size `reduction_dimension`.\n",
        "\n",
        "* Clustering using [**Gaussian Mixture Models (GMM)**](https://scikit-learn.org/stable/modules/mixture.html). The model is fit using Expectation-Maximization algorithm. Documents are assigned to clusters based on a probability threshold; a document can belong to multiple clusters if their probability exceeds the threshold.\n",
        "\n",
        "  The optimal number of clusters is determined using [BIC (Bayesian Information Criterion)](https://en.wikipedia.org/wiki/Bayesian_information_criterion)\n",
        "\n",
        "The clustering is performed through the following recursive process:\n",
        "\n",
        "1. Global dimension reduction and clustering.\n",
        "    \n",
        "    First, the initial embeddings are reduced by UMAP with target embedding size of `reduction_dimension` and `n_neighbors` equal  to the square root of total number of text chunks, and GMM clustering is applied to the resulting vectors. We'll refer to the clusters created as this stage as to global clusters.\n",
        "    \n",
        "    Now, if a global cluster's contains more chunks than dimensionality parameter `reduction_dimension`, this cluster is further subdivided through the local dimension reduction and clustering.\n",
        "    \n",
        "2. Local dimension reduction and clustering\n",
        "    \n",
        "    This is done independently for each large global cluster. First, the original embeddings are reduced by UMAP with target embedding size of `reduction_dimension` and `n_neighbors` equal `reduction_dimension`. Then, each global cluster is again subclustered with GMM.\n",
        "    \n",
        "3. Recursive clustering\n",
        "    Finally, if the total token length of a local cluster exceeds `max_length_in_cluster`, the whole clustering process is recursively reapplied to its text chunks. This continues until all clusters have a total token length within the specified limit.\n",
        "\n",
        "The key advantage of this approach is that it:\n",
        "- Handles high-dimensional data efficiently through UMAP reduction\n",
        "- Automatically determines the optimal number of clusters using BIC\n",
        "- Handles large datasets efficiently by recursively subdividing clusters when their combined text length exceeds the model's token limit\n",
        "- Allows for soft clustering (documents can belong to multiple clusters)\n",
        "- The hierarchical clustering approach enables analysis at multiple levels, capturing both high-level topics and granular subject matter relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-b2nNGOar4a"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    \"\"\"\n",
        "    Represents a node in the hierarchical tree structure.\n",
        "\n",
        "    Attributes:\n",
        "        text (str): The text content of the node\n",
        "        index (int): Unique identifier for the node\n",
        "        children (Set[int]): Set of indices of child nodes\n",
        "        embeddings: Vector representation of the node's text content\n",
        "        summarization_prompt (Optional[str]): Template for generating summaries of this node\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, index: int, children: Set[int], embeddings, summarization_prompt: Optional[str] = None) -> None:\n",
        "        self.text = text\n",
        "        self.index = index\n",
        "        self.children = children\n",
        "        self.embeddings = embeddings\n",
        "        self.summarization_prompt = summarization_prompt\n",
        "\n",
        "\n",
        "def print_clusters(clusters):\n",
        "    \"\"\"\n",
        "    Print the contents of document clusters in a formatted way.\n",
        "\n",
        "    Args:\n",
        "        clusters (List[List[Node]]): List of clusters, where each cluster is a list of Node objects\n",
        "    \"\"\"\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        print()\n",
        "        print(f\"Cluster {i}:\")\n",
        "        for node in cluster:\n",
        "            print(f\"Node {node.index}: {format_text(node.text)}\")\n",
        "            print()\n",
        "        print(\"--------------------------------\")\n",
        "\n",
        "\n",
        "def perform_recursive_clustering(\n",
        "    lance_table: lancedb.table.LanceTable,\n",
        "    cluster_func: Callable,\n",
        "    max_length_in_cluster: int = 3500,\n",
        "    tokenizer=tiktoken.get_encoding(TOKENIZER_NAME),\n",
        "    reduction_dimension: int = 10,\n",
        "    threshold: float = 0.1,\n",
        "    verbose: bool = False,\n",
        "    random_state: int = RANDOM_SEED,\n",
        ") -> List[List[Node]]:\n",
        "    \"\"\"\n",
        "    Recursively clusters documents into a hierarchical structure based on their embeddings.\n",
        "\n",
        "    Args:\n",
        "        lance_table (lancedb.table.LanceTable): Table containing document data\n",
        "        cluster_func (Callable): Function to perform clustering\n",
        "        max_length_in_cluster (int): Maximum total token length allowed in a single cluster\n",
        "        tokenizer: Tokenizer instance for calculating text lengths\n",
        "        reduction_dimension (int): Minimum number of nodes required to continue clustering\n",
        "        threshold (float): Clustering probability threshold\n",
        "        verbose (bool): Whether to print detailed logging information\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        List[List[Node]]: List of clusters, where each cluster contains related Node objects\n",
        "    \"\"\"\n",
        "\n",
        "    # Suppress all warnings for this function\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    df = lance_table.to_pandas()\n",
        "    nodes = df_to_nodes(df)\n",
        "    apply_random_seed(random_state)\n",
        "    embeddings = np.array([node.embeddings for node in nodes])\n",
        "\n",
        "    clusters = perform_clustering_impl(\n",
        "        embeddings, reduction_dimension=reduction_dimension, threshold=threshold, random_state=random_state, cluster_func=cluster_func\n",
        "    )\n",
        "\n",
        "    node_clusters = []\n",
        "\n",
        "    unique_labels = sorted(set(np.concatenate(clusters)))\n",
        "\n",
        "    for label in unique_labels:\n",
        "        indices = sorted([i for i, cluster in enumerate(clusters) if label in cluster])\n",
        "        cluster_nodes = [nodes[i] for i in indices]\n",
        "\n",
        "        if len(cluster_nodes) == 1:\n",
        "            node_clusters.append(cluster_nodes)\n",
        "            continue\n",
        "\n",
        "        total_length = sum(\n",
        "            [len(tokenizer.encode(node.text)) for node in cluster_nodes]\n",
        "        )\n",
        "\n",
        "        if total_length > max_length_in_cluster:\n",
        "            if verbose:\n",
        "                logging.info(\n",
        "                    f\"reclustering cluster with {len(cluster_nodes)} nodes\"\n",
        "                )\n",
        "            node_clusters.extend(\n",
        "                perform_recursive_clustering(\n",
        "                    cluster_nodes,\n",
        "                    cluster_func=cluster_func,\n",
        "                    max_length_in_cluster=max_length_in_cluster,\n",
        "                    tokenizer=tokenizer,\n",
        "                    reduction_dimension=reduction_dimension,\n",
        "                    threshold=threshold,\n",
        "                    verbose=verbose,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            node_clusters.append(cluster_nodes)\n",
        "\n",
        "    return node_clusters\n",
        "\n",
        "\n",
        "def perform_clustering_impl(\n",
        "    embeddings: np.ndarray,\n",
        "    reduction_dimension: int,\n",
        "    threshold: float,\n",
        "    cluster_func: Callable,\n",
        "    verbose: bool = False,\n",
        "    random_state: int = RANDOM_SEED,\n",
        ") -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Implements two-level hierarchical clustering of document embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Document embedding vectors\n",
        "        reduction_dimension (int): Dimension for embedding reduction\n",
        "        threshold (float): Clustering probability threshold\n",
        "        cluster_func (Callable): Function to perform clustering\n",
        "        verbose (bool): Whether to print detailed logging information\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        List[np.ndarray]: List of cluster assignments for each document\n",
        "    \"\"\"\n",
        "    reduced_embeddings_global = global_cluster_embeddings(\n",
        "        embeddings,\n",
        "        min(reduction_dimension, len(embeddings) -2),\n",
        "        random_state=random_state,\n",
        "        n_neighbors=int((len(embeddings) - 1) ** 0.5)\n",
        "    )\n",
        "    global_clusters, n_global_clusters = cluster_func(\n",
        "        reduced_embeddings_global, threshold, random_state=random_state\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        logging.info(f\"Global Clusters: {n_global_clusters}\")\n",
        "\n",
        "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
        "    total_clusters = 0\n",
        "\n",
        "    for i in range(n_global_clusters):\n",
        "        global_cluster_embeddings_ = embeddings[\n",
        "            np.array([i in gc for gc in global_clusters])\n",
        "        ]\n",
        "        if verbose:\n",
        "            logging.info(\n",
        "                f\"Nodes in Global Cluster {i}: {len(global_cluster_embeddings_)}\"\n",
        "            )\n",
        "        if len(global_cluster_embeddings_) == 0:\n",
        "            continue\n",
        "        if len(global_cluster_embeddings_) <= reduction_dimension + 1:\n",
        "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
        "            n_local_clusters = 1\n",
        "        else:\n",
        "            reduced_embeddings_local = local_cluster_embeddings(\n",
        "                global_cluster_embeddings_,\n",
        "                reduction_dimension,\n",
        "                random_state=random_state,\n",
        "                n_neighbors=reduction_dimension\n",
        "            )\n",
        "            local_clusters, n_local_clusters = cluster_func(\n",
        "                reduced_embeddings_local, threshold, random_state=random_state\n",
        "            )\n",
        "\n",
        "        if verbose:\n",
        "            logging.info(f\"Local Clusters in Global Cluster {i}: {n_local_clusters}\")\n",
        "\n",
        "        for j in range(n_local_clusters):\n",
        "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
        "                np.array([j in lc for lc in local_clusters])\n",
        "            ]\n",
        "            indices = np.where(\n",
        "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
        "            )[1]\n",
        "            for idx in indices:\n",
        "                all_local_clusters[idx] = np.append(\n",
        "                    all_local_clusters[idx], j + total_clusters\n",
        "                )\n",
        "\n",
        "        total_clusters += n_local_clusters\n",
        "\n",
        "    if verbose:\n",
        "        logging.info(f\"Total Clusters: {total_clusters}\")\n",
        "    return all_local_clusters\n",
        "\n",
        "\n",
        "def local_cluster_embeddings(\n",
        "    embeddings: np.ndarray, reduction_dimension: int, n_neighbors: int = 10, metric: str = \"cosine\", random_state: int = RANDOM_SEED\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Reduces dimensionality of embeddings for local clustering using UMAP.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Document embedding vectors\n",
        "        reduction_dimension (int): Target dimension for reduction\n",
        "        n_neighbors (int): Number of neighbors to consider in UMAP\n",
        "        metric (str): Distance metric for UMAP\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Reduced dimension embeddings\n",
        "    \"\"\"\n",
        "    reduced_embeddings = umap.UMAP(\n",
        "        n_neighbors=n_neighbors, n_components=reduction_dimension, metric=metric, random_state=random_state\n",
        "    ).fit_transform(embeddings)\n",
        "    return reduced_embeddings\n",
        "\n",
        "\n",
        "def global_cluster_embeddings(\n",
        "    embeddings: np.ndarray,\n",
        "    reduction_dimension: int,\n",
        "    n_neighbors: Optional[int] = None,\n",
        "    metric: str = \"cosine\",\n",
        "    random_state: int = RANDOM_SEED\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Reduces dimensionality of embeddings for global clustering using UMAP.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Document embedding vectors\n",
        "        reduction_dimension (int): Target dimension for reduction\n",
        "        n_neighbors (Optional[int]): Number of neighbors to consider in UMAP. If None, calculated based on dataset size\n",
        "        metric (str): Distance metric for UMAP\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Reduced dimension embeddings\n",
        "    \"\"\"\n",
        "    if n_neighbors is None:\n",
        "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
        "\n",
        "    reduced_embeddings = umap.UMAP(\n",
        "        n_neighbors=n_neighbors, n_components=reduction_dimension, metric=metric, random_state=random_state\n",
        "    ).fit_transform(embeddings)\n",
        "    return reduced_embeddings\n",
        "\n",
        "\n",
        "def df_to_nodes(\n",
        "    df,\n",
        "    text_column='text',\n",
        "    vector_column='vector',\n",
        "    children_column='children',\n",
        "    id_column='id'\n",
        "):\n",
        "    \"\"\"\n",
        "    Converts a pandas DataFrame to a list of Node objects.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing document data\n",
        "        text_column (str): Name of column containing document text\n",
        "        vector_column (str): Name of column containing document embeddings\n",
        "        children_column (str): Name of column containing child node indices\n",
        "        id_column (str): Name of column containing node IDs\n",
        "\n",
        "    Returns:\n",
        "        List[Node]: List of Node objects created from DataFrame rows\n",
        "    \"\"\"\n",
        "    nodes = []\n",
        "    for i, (_, row) in enumerate(df.iterrows()):\n",
        "        node = Node(\n",
        "            text=row[text_column],\n",
        "            index=row[id_column],\n",
        "            children=row[children_column],\n",
        "            embeddings=row[vector_column]\n",
        "        )\n",
        "        nodes.append(node)\n",
        "\n",
        "    return nodes\n",
        "\n",
        "\n",
        "def get_optimal_clusters(\n",
        "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Determines optimal number of clusters using Bayesian Information Criterion (BIC).\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Document embedding vectors\n",
        "        max_clusters (int): Maximum number of clusters to consider\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        int: Optimal number of clusters based on minimum BIC score\n",
        "    \"\"\"\n",
        "    max_clusters = min(max_clusters, len(embeddings))\n",
        "    n_clusters = np.arange(1, max_clusters)\n",
        "    bics = []\n",
        "    for n in n_clusters:\n",
        "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
        "        gm.fit(embeddings)\n",
        "        bics.append(gm.bic(embeddings))\n",
        "    optimal_clusters = n_clusters[np.argmin(bics)]\n",
        "    return optimal_clusters\n",
        "\n",
        "\n",
        "def GMM_cluster(\n",
        "    embeddings: np.ndarray,\n",
        "    threshold: float,\n",
        "    random_state: int = 0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Clusters documents using Gaussian Mixture Model.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Document embedding vectors\n",
        "        threshold (float): Probability threshold for cluster assignment\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], int]: Tuple containing:\n",
        "            - List of cluster assignments for each document\n",
        "            - Number of clusters found\n",
        "    \"\"\"\n",
        "    n_clusters = get_optimal_clusters(embeddings)\n",
        "\n",
        "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
        "    gm.fit(embeddings)\n",
        "    probs = gm.predict_proba(embeddings)\n",
        "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
        "\n",
        "    return labels, n_clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8dCQKbPar4d"
      },
      "source": [
        "Once clusters are found we can see how documents were split into them. First by original clustering function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWaPmeonar4e",
        "outputId": "1959df0a-993a-4db0-8f70-e3b4835f6b41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cluster 0:\n",
            "Node 0: In the quaint coastal town of Bellmare a curious ailment known as \"Pigeon\n",
            "Blessing\" emerged seemingly overnight Initially townsfolk celebrated it\n",
            "believing it to be a sign of good fortune—hence the unusual name However as\n",
            "cases multiplied excitement turned to concernThe first recorded case was Evelyn\n",
            "Carter an elderly woman known locally for feeding pigeons in Bellmare's central\n",
            "plaza\n",
            "\n",
            "Node 1: She began exhibiting peculiar yet benign symptoms an uncanny sense of direction\n",
            "increased visual acuity and most curiously a compulsive need to coo softly\n",
            "particularly during the twilight hours Neighbors joked kindly about Evelyn's\n",
            "newfound \"birdsong\" but soon others experienced similar changesSymptoms\n",
            "progressed gradually typically beginning with heightened senses and remarkable\n",
            "navigation skills but eventually escalating into more pronounced behavioral\n",
            "transformations\n",
            "\n",
            "Node 2: Patients developed subtle feather-like patterns across their shoulders and upper\n",
            "back visible only under specific lighting Over several weeks the affected would\n",
            "display increasingly bird-like behavior such as head tilting rhythmic bobbing\n",
            "and a distinct preference for elevated locationsLocal physician Dr Clara Novak\n",
            "was initially baffled but intrigued She documented each case meticulously The\n",
            "affliction appeared harmless physically causing no pain or discomfort beyond\n",
            "mild social embarrassment\n",
            "\n",
            "Node 7: The therapy emphasized gentle reintegration reminding patients of their human\n",
            "identity and reestablishing typical social interactions This breakthrough\n",
            "allowed Bellmare to return to normal life and recover from Pigeon BlessingWithin\n",
            "months Bellmare saw significant improvements Pigeon Blessing receded from a\n",
            "troubling epidemic to a quirky chapter in the town's history\n",
            "\n",
            "Node 8: Evelyn Carter now fully recovered continued feeding pigeons but wore a\n",
            "protective mask and gloves humorously cautioning others \"Careful now—too much of\n",
            "a good thing can turn you feather-brained\"Bellmare residents learned to balance\n",
            "coexistence with nature aware now of the delicate equilibrium that sustains\n",
            "their peaceful communityPigeons scientifically classified within the family\n",
            "Columbidae are widespread birds known for their adaptability to various\n",
            "environments from urban centers to rural landscapes\n",
            "\n",
            "--------------------------------\n",
            "\n",
            "Cluster 1:\n",
            "Node 15: This spell is most effective on creatures which have a large damage range and at\n",
            "Advanced and Expert level stacks of troops with low damage but large numbers of\n",
            "creatures in themThe +1 extra damage bonus from Advanced Water Magic is more\n",
            "beneficial to units that normally deal low damage as it's greater percentage of\n",
            "their base damage so it's even more useful to low level units with high numbers\n",
            "and large damage bracket\n",
            "\n",
            "Node 16: Many 1st level units deal 1-3 damage values - Pikemen Troglodytes Sprites\n",
            "(Skeletons too but as Undead they cannot be Blessed) Harpies & Harpy Hags with\n",
            "1-4 damage are perhaps most affected as Advanced Bless causes them to deal 5\n",
            "damage instead of 1-4 average 25 doubling their output\n",
            "\n",
            "Node 17: Air Elementals & Storm Elementals deal similarly proportioned 2-8 damageBecause\n",
            "of its mechanic normal and basic Bless is useless to cast on troops with fixed\n",
            "damage value Angels Nagas Peasants Enchanters Rust Dragons and Automatons Horn\n",
            "of the Abyss Advanced Bless has a very minor effect as 1 point of damage is more\n",
            "or less a negligible amount on such troops\n",
            "\n",
            "Node 18: With the exception of peasants where +1 basically doubles their damage output\n",
            "This boost could be a game changer in numbers massive enough However getting to\n",
            "that point is only reasonable on paper and even then it is really hard to keep\n",
            "them alive\n",
            "\n",
            "--------------------------------\n",
            "\n",
            "Cluster 2:\n",
            "Node 3: However psychological impacts became pronounced as affected individuals\n",
            "struggled with identity and social interactionsDr Novak's breakthrough came\n",
            "after collaborating with biologist Liam Greer who noted a peculiar fungus\n",
            "present in local pigeon droppings The fungus named Columba benedicta released\n",
            "spores that affected the human nervous system when inhaled in significant\n",
            "quantities subtly rewiring brain function and mimicking avian neurological\n",
            "traits\n",
            "\n",
            "Node 4: That is how it became that Pigeon Blessing is nothing else but a fungus columba\n",
            "benedictaBut how to heal from columba benedictaBefore answering this question\n",
            "let's first understand what fungus is Fungus is a diverse group of eukaryotic\n",
            "organisms distinct from plants animals and bacteria categorized into their own\n",
            "kingdom called Fungi\n",
            "\n",
            "Node 5: With a clear cause identified treatment of columba benedicta began with\n",
            "antifungal medications tailored to counteract the spores' neurological effects\n",
            "To counteract columba benedicta Dr Novak introduced \"Ornizol\" an oral medication\n",
            "combined with cognitive-behavioral therapy (CBT) assisting patients in\n",
            "retraining their brain pathways and reclaiming their human behaviors and social\n",
            "norms\n",
            "\n",
            "Node 6: Additional effective treatments included \"Aviaclear\" a nasal spray designed to\n",
            "neutralize spores before they could affect the nervous system and \"Featheroff\" a\n",
            "topical cream used to eliminate feather-like skin manifestations For severe\n",
            "neurological cases Dr Novak prescribed \"Neurocol\" an intravenous antifungal\n",
            "agent specifically designed to rapidly reverse extensive neurological rewiring\n",
            "caused by prolonged exposureCommunity outreach programs provided psychological\n",
            "support reducing stigma around the condition\n",
            "\n",
            "--------------------------------\n",
            "\n",
            "Cluster 3:\n",
            "Node 9: The domestic pigeon (Columba livia domestica) derived from the wild rock pigeon\n",
            "(Columba livia) exemplifies remarkable biological and behavioral traits that\n",
            "have allowed it to thrive globallyAnatomy and PhysiologyPigeons have robust\n",
            "streamlined bodies designed for efficient flight Their powerful flight muscles\n",
            "particularly the pectoralis major enable rapid and sustained flight Wings are\n",
            "broad and rounded optimizing both lift and maneuverability\n",
            "\n",
            "Node 10: The feathers are densely packed waterproof and provide insulationTheir vision is\n",
            "particularly advanced with pigeons capable of seeing ultraviolet wavelengths\n",
            "This enhanced vision aids in navigation and foraging allowing pigeons to\n",
            "identify subtle patterns and colors imperceptible to humansBehavior and\n",
            "EcologyPigeons exhibit strong social behaviors typically forming large flocks\n",
            "Their social structure revolves around communal roosting sites and collective\n",
            "feeding grounds\n",
            "\n",
            "Node 11: Pigeons communicate primarily through vocalizations and body movements with\n",
            "cooing being the most recognizable soundDietarily pigeons are primarily\n",
            "granivorous feeding on seeds grains and occasionally fruits and insects Their\n",
            "adaptability extends to diet enabling survival in diverse\n",
            "habitatsReproductionPigeons are monogamous and exhibit strong pair bonds often\n",
            "lasting several breeding seasons or for life Both sexes participate actively in\n",
            "nesting incubation and rearing of chicks\n",
            "\n",
            "Node 12: Females typically lay two eggs per clutch which hatch after approximately 17–19\n",
            "days of incubationChicks known as squabs are initially fed \"pigeon milk\" a\n",
            "nutrient-rich secretion produced in the crops of both parents before\n",
            "transitioning to solid food Squabs fledge within 30 days and quickly integrate\n",
            "into the flockNavigation and Homing AbilityOne of the most fascinating aspects\n",
            "of pigeon biology is their exceptional homing instinct\n",
            "\n",
            "Node 13: This ability is thought to involve multiple mechanisms including magnetic field\n",
            "detection solar positioning and recognition of environmental landmarks Their\n",
            "neurological structures particularly within the hippocampus are highly developed\n",
            "to support spatial memory and navigationEcological Significance and Human\n",
            "InteractionPigeons serve significant ecological roles such as seed dispersion\n",
            "and providing prey for urban-adapted predators\n",
            "\n",
            "Node 14: Human interactions with pigeons are diverse ranging from historical uses such as\n",
            "messenger birds and food sources to contemporary roles as indicators of urban\n",
            "ecological healthIn modern urban environments pigeons are sometimes viewed\n",
            "negatively due to their abundance and potential as disease vectors However\n",
            "ongoing research continues to reveal the complexity of pigeon biology and\n",
            "underscores their importance within ecosystems and human cultures worldwide\n",
            "Pigeons are rarely considered as a blessing but rather as a nuisanceThe Bless\n",
            "spell counters the Curse spell\n",
            "\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "clusters = perform_recursive_clustering(\n",
        "    lance_table,\n",
        "    random_state=RANDOM_SEED,\n",
        "    cluster_func=GMM_cluster\n",
        ")\n",
        "print_clusters(clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azsDmkYmF9_P"
      },
      "source": [
        "We can see that the clustering results above are quite coherent and interpretable:\n",
        "\n",
        "Cluster 0: A general narrative about the Pigeon Blessing illness\n",
        "\n",
        "Cluster 1: Description of the Bless spell from a video game\n",
        "\n",
        "Cluster 2: Details on the symptoms, risks, and causes of Pigeon Blessing\n",
        "\n",
        "Cluster 3: General information about pigeons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luaHtitDar4p"
      },
      "source": [
        "## Step 2. Create database of summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92aT_NFYar4p"
      },
      "source": [
        "Now that we have a hierarchical clustering, we'll summarize each cluster.\n",
        "\n",
        "**Note**. We only summarize clusters which are at the bottom of the cluster hierarchy. That is:\n",
        "\n",
        "- If a global cluster was small and we didn't subdivide it, we summarize this cluster\n",
        "- If a global cluster was too large and we subdivided it into local clusters, we summarize each local cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVveC9B4ar4p"
      },
      "outputs": [],
      "source": [
        "def summarize(\n",
        "    client,\n",
        "    text_to_summarize,\n",
        "    model_name,\n",
        "    temperature,\n",
        "    seed,\n",
        "    max_length=MAX_LENGTH_TO_SUMMARIZE\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a concise MAX_SUMMARIZATION_LENGTH-word summary of the provided text using an LLM.\n",
        "\n",
        "    Args:\n",
        "        client: The LLM client instance used for generating summaries\n",
        "        text_to_summarize (str): The input text that needs to be summarized\n",
        "        model_name (str): Name of the LLM model to use for summarization\n",
        "        temperature (float): Controls randomness in the model's output. Higher values (e.g., 1.0) make output more random,\n",
        "                           lower values (e.g., 0.1) make it more deterministic\n",
        "        seed (int): Random seed for reproducible results\n",
        "        max_length (int, optional): Maximum length of input text to summarize. Defaults to MAX_LENGTH_TO_SUMMARIZE.\n",
        "\n",
        "    Returns:\n",
        "        str: A MAX_SUMMARIZATION_LENGTH-word summary of the input text containing key details\n",
        "\n",
        "    Note:\n",
        "        - If input text exceeds max_length, it will be truncated from the start\n",
        "        - Uses format_text() helper function to format output for display\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure that the text is not too long\n",
        "    text_split = text_to_summarize.split(\" \")[::-1]\n",
        "    reduced_text_to_summarize = \"\"\n",
        "    while len(reduced_text_to_summarize) < max_length:\n",
        "        if len(text_split) == 0:\n",
        "            break\n",
        "        reduced_text_to_summarize += \" \" + text_split.pop()\n",
        "    text_to_summarize = reduced_text_to_summarize\n",
        "\n",
        "    print(\"Text to summarize:\", format_text(text_to_summarize))\n",
        "    prompt = (\n",
        "        f\"Write a summary of the following, including as many key details as possible: \"\n",
        "        f\"{text_to_summarize}. \"\n",
        "        f\"Make sure that summary is {MAX_SUMMARIZATION_LENGTH} words long.\")\n",
        "    completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            model=model_name,\n",
        "            temperature=temperature,\n",
        "            seed=seed\n",
        "            )\n",
        "\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "\n",
        "def summarize_cluster(cluster_index, client, cluster):\n",
        "    \"\"\"\n",
        "    Generates a summary for a cluster of text documents.\n",
        "\n",
        "    Args:\n",
        "        cluster_index (int): The index identifying the current cluster\n",
        "        client: The LLM client instance used for generating summaries\n",
        "        cluster (list): List of node objects representing documents in the cluster.\n",
        "                       Each node is expected to have a 'text' attribute containing the document text.\n",
        "\n",
        "    Returns:\n",
        "        str: A summary of all documents in the cluster\n",
        "\n",
        "    Note:\n",
        "        - Concatenates all texts in the cluster with newline separation\n",
        "        - Uses the summarize() function with predefined SUMMARIZATION_MODEL, TEMPERATURE and RANDOM_SEED\n",
        "        - Prints formatted cluster context and summary with visual separators\n",
        "    \"\"\"\n",
        "\n",
        "    context = \"\"\n",
        "    print(\"--------------------------------\")\n",
        "    for i, node in enumerate(cluster):\n",
        "        context += f\"{node.text}\\n\"\n",
        "    print(f\"Context to summarize for cluster {cluster_index}:\\n\")\n",
        "\n",
        "    summary = summarize(\n",
        "        client,\n",
        "        context,\n",
        "        model_name=SUMMARIZATION_MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        seed=RANDOM_SEED\n",
        "    )\n",
        "    print(f\"Summary for cluster {cluster_index}:\\n\")\n",
        "    print(format_text(summary))\n",
        "    print(\"--------------------------------\")\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_5yheDKar4r"
      },
      "source": [
        "Now we can see summaries for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkgOez5jar4r",
        "outputId": "7bd750e7-25e0-46cd-f745-ceb808326b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Context to summarize for cluster 0:\n",
            "\n",
            "Text to summarize: In the quaint coastal town of Bellmare a curious ailment known as \"Pigeon\n",
            "Blessing\" emerged seemingly overnight Initially townsfolk celebrated it\n",
            "believing it to be a sign of good fortune—hence the unusual name However as\n",
            "cases multiplied excitement turned to concernThe first recorded case was Evelyn\n",
            "Carter an elderly woman known locally for feeding pigeons in Bellmare's central\n",
            "plaza She began exhibiting peculiar yet benign symptoms an uncanny sense of\n",
            "direction increased visual acuity and most curiously a compulsive need to coo\n",
            "softly particularly during the twilight hours Neighbors joked kindly about\n",
            "Evelyn's newfound \"birdsong\" but soon others experienced similar changesSymptoms\n",
            "progressed gradually typically beginning with heightened senses and remarkable\n",
            "navigation skills but eventually escalating into more pronounced behavioral\n",
            "transformations Patients developed subtle feather-like patterns across their\n",
            "shoulders and upper back visible only under specific lighting Over several\n",
            "Summary for cluster 0:\n",
            "\n",
            "In Bellmare a coastal town \"Pigeon Blessing\" afflicted residents with a\n",
            "mysterious ailment Initially it manifested as heightened senses and improved\n",
            "navigation but eventually patients developed feather-like patterns and a\n",
            "compulsion to coo softly The condition seemingly spreading rapidly sparked\n",
            "concern as it progressed from benign to more pronounced\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Context to summarize for cluster 1:\n",
            "\n",
            "Text to summarize: This spell is most effective on creatures which have a large damage range and at\n",
            "Advanced and Expert level stacks of troops with low damage but large numbers of\n",
            "creatures in themThe +1 extra damage bonus from Advanced Water Magic is more\n",
            "beneficial to units that normally deal low damage as it's greater percentage of\n",
            "their base damage so it's even more useful to low level units with high numbers\n",
            "and large damage bracket Many 1st level units deal 1-3 damage values - Pikemen\n",
            "Troglodytes Sprites (Skeletons too but as Undead they cannot be Blessed) Harpies\n",
            "& Harpy Hags with 1-4 damage are perhaps most affected as Advanced Bless causes\n",
            "them to deal 5 damage instead of 1-4 average 25 doubling their output Air\n",
            "Elementals & Storm Elementals deal similarly proportioned 2-8 damageBecause of\n",
            "its mechanic normal and basic Bless is useless to cast on troops with fixed\n",
            "damage value Angels Nagas Peasants Enchanters Rust Dragons and Automatons Horn\n",
            "of the Abyss Advanced Bless has a very minor\n",
            "Summary for cluster 1:\n",
            "\n",
            "Advanced Water Magic blesses troops with a damage range boosting their damage\n",
            "output significantly It's most effective on low-damage units with high numbers\n",
            "like Pikemen Harpies and Air Elementals While basic Bless is useless for fixed\n",
            "damage units Advanced Bless offers a substantial boost to low-damage units\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Context to summarize for cluster 2:\n",
            "\n",
            "Text to summarize: However psychological impacts became pronounced as affected individuals\n",
            "struggled with identity and social interactionsDr Novak's breakthrough came\n",
            "after collaborating with biologist Liam Greer who noted a peculiar fungus\n",
            "present in local pigeon droppings The fungus named Columba benedicta released\n",
            "spores that affected the human nervous system when inhaled in significant\n",
            "quantities subtly rewiring brain function and mimicking avian neurological\n",
            "traits That is how it became that Pigeon Blessing is nothing else but a fungus\n",
            "columba benedictaBut how to heal from columba benedictaBefore answering this\n",
            "question let's first understand what fungus is Fungus is a diverse group of\n",
            "eukaryotic organisms distinct from plants animals and bacteria categorized into\n",
            "their own kingdom called FungiWith a clear cause identified treatment of columba\n",
            "benedicta began with antifungal medications tailored to counteract the spores'\n",
            "neurological effects To counteract columba benedicta Dr Novak introduced\n",
            "Summary for cluster 2:\n",
            "\n",
            "A fungus *C benedicta* found in pigeon droppings causes neurological changes in\n",
            "humans leading to identity and social issues Dr Novak discovered this fungus and\n",
            "its impact on the nervous system Treatment involves antifungal medications\n",
            "specifically designed to counteract the fungus's neurological effects\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Context to summarize for cluster 3:\n",
            "\n",
            "Text to summarize: The domestic pigeon (Columba livia domestica) derived from the wild rock pigeon\n",
            "(Columba livia) exemplifies remarkable biological and behavioral traits that\n",
            "have allowed it to thrive globallyAnatomy and PhysiologyPigeons have robust\n",
            "streamlined bodies designed for efficient flight Their powerful flight muscles\n",
            "particularly the pectoralis major enable rapid and sustained flight Wings are\n",
            "broad and rounded optimizing both lift and maneuverability The feathers are\n",
            "densely packed waterproof and provide insulationTheir vision is particularly\n",
            "advanced with pigeons capable of seeing ultraviolet wavelengths This enhanced\n",
            "vision aids in navigation and foraging allowing pigeons to identify subtle\n",
            "patterns and colors imperceptible to humansBehavior and EcologyPigeons exhibit\n",
            "strong social behaviors typically forming large flocks Their social structure\n",
            "revolves around communal roosting sites and collective feeding grounds Pigeons\n",
            "communicate primarily through vocalizations and body movements\n",
            "Summary for cluster 3:\n",
            "\n",
            "Domestic pigeons descended from wild rock pigeons are highly adapted for flight\n",
            "and survival Their streamlined bodies powerful flight muscles and dense\n",
            "waterproof feathers enable efficient flight Advanced vision including the\n",
            "ability to see ultraviolet light aids in navigation and foraging Pigeons live in\n",
            "large flocks communicating through vocalizations and body language\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "summarizied_docs = []\n",
        "for i, cluster in enumerate(clusters):\n",
        "    summary = summarize_cluster(i, client, cluster)\n",
        "    summarizied_docs.append(\n",
        "        {\n",
        "            \"text\": summary,\n",
        "            \"children\": [node.index for node in cluster],\n",
        "            \"id\": i\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jFrbL_tar4s"
      },
      "source": [
        "These summaries can be converted into a LanceDB database similarly to how we did it for the original documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9SQk1UXar4s"
      },
      "outputs": [],
      "source": [
        "summaries_db_path = \"/tmp/summaries_lancedb\"\n",
        "shutil.rmtree(summaries_db_path, ignore_errors=True)\n",
        "\n",
        "summaries_db = lancedb.connect(summaries_db_path)\n",
        "\n",
        "summaries_lance_table = summaries_db.create_table(\n",
        "    \"transformer_docs\",\n",
        "    mode='overwrite',\n",
        "    schema=BasicSchema\n",
        ")\n",
        "\n",
        "summaries_lance_table.add(\n",
        "    summarizied_docs,\n",
        "    on_bad_vectors='drop'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDM7Q3HXar4u"
      },
      "source": [
        "## Raptor retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_eBpRFxar4u"
      },
      "source": [
        "![RAPTOR Retrieval Process](https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic3/raptor-querying.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoxWMELhar4v"
      },
      "source": [
        "Third, let's implement RAPTOR retrieval by using a database of original documents as well as summarizations (**\"A. Tree traversal retrieval\"** from the picture above).\n",
        "\n",
        "**Your task** is to complete its code, filling the `# <YOUR CODE HERE>` portion.\n",
        "\n",
        "Hint for implementing RAPTOR retrieval:\n",
        "1. We have two databases:\n",
        "   - `summaries_lance_table` - Contains summarized documents with \"children\" field listing IDs of original docs\n",
        "   - `lance_table` - Contains the original documents\n",
        "\n",
        "2. The retrieval process should:\n",
        "\n",
        "   a. First search the summaries database to find relevant summary documents\n",
        "\n",
        "   b. Extract the child document IDs from the matched summaries\n",
        "\n",
        "   c. Then search the original documents database, but only among those child documents (you can use `where` argument for `search_table` function)\n",
        "\n",
        "   d. Return both the relevant summaries and filtered original documents\n",
        "\n",
        "3. This two-stage retrieval helps focus the search on the most relevant document clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI43a6Iwar4v"
      },
      "source": [
        "**Comment about BasicSchema class:**\n",
        "\n",
        "In contrast to Standard RAG you can now take advantage of the field \"children\" of the `BasicSchema`.\n",
        "\n",
        "For example, it can store IDs of original documents that were used to create each summary in RAPTOR pipeline, enabling RAPTOR to track relationships between summaries and source documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LF9kd2Oar4w"
      },
      "outputs": [],
      "source": [
        "def retrieve_docs(lance_table, summaries_lance_table, query, limit=TOP_K):\n",
        "    \"\"\"\n",
        "    Implements RAPTOR's two-stage retrieval process by searching both summary and original document databases.\n",
        "\n",
        "    Args:\n",
        "        lance_table: LanceDB table containing the original documents\n",
        "        summaries_lance_table: LanceDB table containing document summaries with child document references\n",
        "        query: String containing the search query\n",
        "        limit: Maximum number of documents to retrieve (default: TOP_K)\n",
        "\n",
        "    Returns:\n",
        "        str: Concatenated context string containing both retrieved summaries and original documents\n",
        "\n",
        "    The function performs retrieval in two stages:\n",
        "    1. Searches the summaries database to find relevant summary documents\n",
        "    2. Extracts child document IDs from matched summaries\n",
        "    3. Searches original documents database, filtered to only match the extracted child documents\n",
        "    4. Prints and returns both summaries and filtered original documents as context\n",
        "\n",
        "    The returned context string contains formatted text from both summary and original documents,\n",
        "    with clear separation between the two types of retrieved content.\n",
        "    \"\"\"\n",
        "\n",
        "    # INSERT CODE\n",
        "    summarized_docs = search_table(summaries_lance_table, query, limit=limit)\n",
        "    white_list = []\n",
        "    for doc in summarized_docs:\n",
        "        white_list.extend(doc.children)\n",
        "\n",
        "    white_list_str = \"(\" + \",\".join(map(str, white_list)) + \")\"\n",
        "    original_docs = search_table(lance_table, query, where=f\"id IN {white_list_str}\", limit=limit)\n",
        "    # INSERT CODE\n",
        "\n",
        "    context = \"\"\n",
        "    print(\"--------------------------------\")\n",
        "    print(\"Retrieved summarized docs:\\n\")\n",
        "    for summarized_doc in summarized_docs:\n",
        "        summarized_doc = format_text(summarized_doc.text)\n",
        "        context += summarized_doc\n",
        "        print(summarized_doc)\n",
        "\n",
        "    print()\n",
        "    print(\"Retrieved original docs:\\n\")\n",
        "    for doc in original_docs:\n",
        "        original_doc = format_text(doc.text)\n",
        "        context += original_doc\n",
        "        print(original_doc)\n",
        "    print(\"--------------------------------\")\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "def prepare_context_maker_raptor(lance_table, summaries_lance_table, limit=TOP_K):\n",
        "    \"\"\"\n",
        "    Creates a context maker function for RAPTOR retrieval.\n",
        "\n",
        "    Args:\n",
        "        lance_table: LanceDB table containing original documents\n",
        "        summaries_lance_table: LanceDB table containing document summaries\n",
        "        limit: Maximum number of documents to retrieve (default: TOP_K)\n",
        "\n",
        "    Returns:\n",
        "        function: A function that takes a query string and returns retrieved context\n",
        "\n",
        "    The returned function (make_context_raptor) encapsulates the RAPTOR retrieval logic,\n",
        "    making it easy to use in higher-level RAG implementations. It maintains consistent\n",
        "    access to the database tables and retrieval limit while allowing different queries\n",
        "    to be processed.\n",
        "\n",
        "    The returned function signature is:\n",
        "        make_context_raptor(query: str) -> str\n",
        "    \"\"\"\n",
        "    def make_context_raptor(query):\n",
        "        return retrieve_docs(lance_table, summaries_lance_table, query, limit=limit)\n",
        "    return make_context_raptor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N7aD3Boar4x"
      },
      "source": [
        "## Answer question with RAPTOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCjUu7B0ar4x"
      },
      "source": [
        "Now we can answer the same question that we could not answer using standard RAG now using RAPTOR's two-stage retrieval approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73X08Y1lar4y",
        "outputId": "a53cecca-9ab5-4fe1-cc3f-4d5d010f478a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Retrieved summarized docs:\n",
            "\n",
            "A fungus *C benedicta* found in pigeon droppings causes neurological changes in\n",
            "humans leading to identity and social issues Dr Novak discovered this fungus and\n",
            "its impact on the nervous system Treatment involves antifungal medications\n",
            "specifically designed to counteract the fungus's neurological effects\n",
            "\n",
            "Retrieved original docs:\n",
            "\n",
            "However psychological impacts became pronounced as affected individuals\n",
            "struggled with identity and social interactionsDr Novak's breakthrough came\n",
            "after collaborating with biologist Liam Greer who noted a peculiar fungus\n",
            "present in local pigeon droppings The fungus named Columba benedicta released\n",
            "spores that affected the human nervous system when inhaled in significant\n",
            "quantities subtly rewiring brain function and mimicking avian neurological\n",
            "traits\n",
            "--------------------------------\n",
            "Question:\n",
            " Was the pigeon disease harmful and what was the cause of it?\n",
            "--------------------------------\n",
            "Retrieved context:\n",
            " A fungus *C benedicta* found in pigeon droppings causes neurological changes\n",
            "inhumans leading to identity and social issues Dr Novak discovered this fungus\n",
            "andits impact on the nervous system Treatment involves antifungal\n",
            "medicationsspecifically designed to counteract the fungus's neurological\n",
            "effectsHowever psychological impacts became pronounced as affected\n",
            "individualsstruggled with identity and social interactionsDr Novak's\n",
            "breakthrough cameafter collaborating with biologist Liam Greer who noted a\n",
            "peculiar funguspresent in local pigeon droppings The fungus named Columba\n",
            "benedicta releasedspores that affected the human nervous system when inhaled in\n",
            "significantquantities subtly rewiring brain function and mimicking avian\n",
            "neurologicaltraits\n",
            "--------------------------------\n",
            "Answer based on context:\n",
            " The fungus *Columba benedicta* was the cause of the disease. It was harmful to humans, causing neurological changes, identity and social issues.\n"
          ]
        }
      ],
      "source": [
        "answer = answer_with_rag(\n",
        "    QUESTION,\n",
        "    client=client,\n",
        "    model_name=ANSWERING_MODEL_NAME,\n",
        "    make_context=prepare_context_maker_raptor(lance_table, summaries_lance_table),\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUID26YKar40"
      },
      "source": [
        "Let's compare it once more to the original answer of RAG that did not use summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsU8m6rAar41",
        "outputId": "472b78c3-67e8-4478-e680-0c5fe35def68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            " Was the pigeon disease harmful and what was the cause of it?\n",
            "--------------------------------\n",
            "Retrieved context:\n",
            " Human interactions with pigeons are diverse ranging from historical uses such as\n",
            "messenger birds and food sources to contemporary roles as indicators of urban\n",
            "ecological healthIn modern urban environments pigeons are sometimes viewed\n",
            "negatively due to their abundance and potential as disease vectors However\n",
            "ongoing research continues to reveal the complexity of pigeon biology and\n",
            "underscores their importance within ecosystems and human cultures worldwide\n",
            "Pigeons are rarely considered as a blessing but rather as a nuisanceThe Bless\n",
            "spell counters the Curse spell\n",
            "--------------------------------\n",
            "Answer based on context:\n",
            " I don't know.\n"
          ]
        }
      ],
      "source": [
        "results = answer_with_rag(\n",
        "    QUESTION,\n",
        "    client=client,\n",
        "    model_name=ANSWERING_MODEL_NAME,\n",
        "    make_context=prepare_context_maker_standard(lance_table),\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlAaZOUUar42"
      },
      "source": [
        "As we can see, the RAPTOR pipeline answers questions even when standard RAG fails due to poor text granularity. In the example above, the standard RAG retrieved a document about pigeons as birds instead of the illness called \"Pigeon Blessing\". That happened because\n",
        "the original database used fixed-size splits and it did not include a single text about all the three parts of the question: the illness, its harm, or its cause. As a result, the retriever found the wrong text about pigeons as the closest match. RAPTOR in contrast collected information about Pigeon Blessing from multiple documents and produced a more accurate summary about the illness, its harm, and its cause. We visualize this below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfpzcx8EF6YW"
      },
      "source": [
        "# Further remarks\n",
        "\n",
        "RAPTOR is very sensitive to original documents' clusterization outcome, especially to the choice of the optimal number of clusters. For GaussianMixture with BICs it works better (clusters documents in 4 clusters that leads to correct answer) than for K-Means with Elbow, Silhouette, or Gaps methods (clusters documents in 1 or 2 clusters that leads to incorrect answer).\n",
        "\n",
        "However, the high dimensionality of vector embeddings poses a challenge for traditional GMMs, as distance metrics can behave poorly in high-dimensional spaces. To address this, we employ Uniform Manifold Approximation and Projection (UMAP), a manifold learning technique for dimensionality reduction. This reduction helps maintain meaningful relationships between embeddings while making the clustering process more robust and computationally efficient.\n",
        "\n",
        "Another key innovation in Raptor's clustering approach is the use of soft clustering, where nodes can belong to multiple clusters without requiring a fixed number of clusters. This flexibility is crucial because text segments often contain information relevant to multiple topics, making their inclusion in multiple summaries necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjApll4nar44"
      },
      "source": [
        "# Practice part"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you encounter any difficulties or simply want to see our solutions, feel free to check the [Solutions notebook](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic3/3.4_raptor_demo_solutions.ipynb)."
      ],
      "metadata": {
        "id": "4lXc8V82d-qN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRC37L43ar4e"
      },
      "source": [
        "## Task 1. Simplified clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNOx1jEbar4f"
      },
      "source": [
        "One can notice that GMM as a clustering algorithm looks a bit too complex. In this task, you'll try to replace it with a simpler clustering algorithm - [k-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
        "\n",
        "Additional things to do:\n",
        "\n",
        "1. Try choosing an optimal value of $k$ using one of the following [evaluation strategies](https://www.youtube.com/watch?v=QLhetMMhNvE):\n",
        "\n",
        "  - **Elbow Method**. Calculate *inertia* - the sum of squared distances between each data point and its assigned centroid for different values of $k$. Plot the inertia against the number of clusters. The optimal $k$ is where the \"elbow\" occurs in the plot - that is where the inertia plot's slope abruptly becomes less steep. This is the point after which adding more clusters doesn't significantly reduce the inertia.\n",
        "\n",
        "  Inertia may be straightforwardly extracted from a fitted KMeans object as `model.inertia_`.\n",
        "\n",
        "  - **Silhouette Method**. Calculates the [*silhouette score*](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) for each data point. The silhouette score measures how similar a point is to its own cluster compared to other clusters. It ranges from $-1$ to $1$, where:\n",
        "    \n",
        "    $1$ means that points are well-clustered\n",
        "    \n",
        "    $0$ means that points are on the boundary between clusters\n",
        "\n",
        "    $-1$ means that points are probably in a wrong cluster\n",
        "  \n",
        "    The optimal $k$ is the one that maximizes the average silhouette score\n",
        "\n",
        "  In a real-world situation you'd want to use *validation dataset* for choosing $k$ and then a separate *test dataset* to score the overall performance of clustering. With a sufficiently large database, you might afford it, but our database is just too small. However, to make your efforts more production-like, we recommend generating some similar data as a validation set.\n",
        "\n",
        "2. Compare the implementation of RAPTOR that uses k-Means to the original one and explain whether the choice of more complex Gaussian Mixture model is justified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE6r6K-par4f"
      },
      "source": [
        "Please implement a simplified version of the clustering algorithm and perform a sensitivity analysis that evaluates:\n",
        "- The impact of different hyperparameter choices on RAPTOR's performance\n",
        "- The influence of the clustering algorithm selection on RAPTOR's results\n",
        "\n",
        "This analysis should help understand how robust RAPTOR is to these variations in the clustering setup.\n",
        "The conclusion for this analysis will be needed in **Conclusions for clustering algorithms**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdGkPd_zar4g"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "def table_to_embeddings(lance_table):\n",
        "    \"\"\"Convert a Lance table to embeddings and nodes.\n",
        "\n",
        "    This function takes a Lance table and converts it to a tuple of embeddings array\n",
        "    and nodes list. The Lance table is first converted to a pandas DataFrame, then\n",
        "    transformed into nodes with embeddings.\n",
        "\n",
        "    Args:\n",
        "        lance_table: A Lance table containing document data\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, List]: A tuple containing:\n",
        "            - np.ndarray: Array of embeddings for each document\n",
        "            - List: List of node objects containing document metadata\n",
        "    \"\"\"\n",
        "    nodes = df_to_nodes(lance_table.to_pandas())\n",
        "    embeddings = np.array([node.embeddings for node in nodes])\n",
        "    return embeddings, nodes\n",
        "\n",
        "\n",
        "def get_optimal_clusters_kmeans(embeddings):\n",
        "    \"\"\"Determine the optimal number of clusters using the elbow method.\n",
        "\n",
        "    This function implements the elbow method by running k-means clustering with\n",
        "    different numbers of clusters and analyzing the inertia (within-cluster sum\n",
        "    of squares) curve to find the optimal number of clusters.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Array of document embeddings to cluster\n",
        "\n",
        "    Returns:\n",
        "        int: The optimal number of clusters determined by the elbow method\n",
        "    \"\"\"\n",
        "    def find_elbow_point(distortions):\n",
        "        \"\"\"Find the elbow point in the distortion curve.\n",
        "\n",
        "        This function calculates the second derivative of the distortion curve\n",
        "        and finds the point of maximum curvature, which represents the elbow point.\n",
        "\n",
        "        Args:\n",
        "            distortions (List[float]): List of distortion values for different k\n",
        "\n",
        "        Returns:\n",
        "            int: The index representing the optimal number of clusters\n",
        "        \"\"\"\n",
        "        # Calculate the second derivative\n",
        "        second_derivative = np.diff(np.diff(distortions))\n",
        "\n",
        "        # Find the point where the second derivative is maximum\n",
        "        elbow_point = np.argmax(second_derivative) + 2  # +2 because we took two differences\n",
        "\n",
        "        return elbow_point\n",
        "\n",
        "    n_clusters = np.arange(1, len(embeddings))\n",
        "    inertias = []\n",
        "    for n in n_clusters:\n",
        "        kmeans = KMeans(n_clusters=n, random_state=RANDOM_SEED)\n",
        "        kmeans.fit(embeddings)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    optimal_n_clusters = find_elbow_point(inertias)\n",
        "    return optimal_n_clusters\n",
        "\n",
        "def perform_kmeans_clustering(lance_table, random_state=RANDOM_SEED, n_clusters=None):\n",
        "    \"\"\"Perform k-means clustering on documents in a Lance table.\n",
        "\n",
        "    This function clusters documents using k-means clustering. It first converts\n",
        "    the Lance table to embeddings, determines the optimal number of clusters,\n",
        "    performs clustering, and organizes documents into their respective clusters.\n",
        "\n",
        "    Args:\n",
        "        lance_table: Lance table containing document data\n",
        "        random_state (int, optional): Random seed for reproducibility. Defaults to RANDOM_SEED.\n",
        "\n",
        "    Returns:\n",
        "        List[List[Node]]: A list of clusters, where each cluster is a list of Node objects\n",
        "            representing the documents in that cluster\n",
        "    \"\"\"\n",
        "    # first convert table to a list of nodes with the following fields: text, index, children, embeddings\n",
        "    embeddings, nodes = table_to_embeddings(lance_table)\n",
        "\n",
        "    if n_clusters is None:\n",
        "        n_clusters = get_optimal_clusters_kmeans(embeddings)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED)\n",
        "    kmeans.fit(embeddings)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    clusters = [[] for _ in range(n_clusters)]\n",
        "    for i, label in enumerate(labels):\n",
        "        clusters[label].append(nodes[i])\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Put your code below]"
      ],
      "metadata": {
        "id": "81C2HhU7dsoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Put your code above]"
      ],
      "metadata": {
        "id": "Laxy26f7dogy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OurSGJhgF9_R"
      },
      "source": [
        "Let's cluster the documents using KMeans, setting n_clusters to the value determined in the previous analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSBt_Di-ar4o"
      },
      "outputs": [],
      "source": [
        "n_clusters_based_on_analysis = None # change this number to the optimal number of clusters determined\n",
        "clusters_kmeans = perform_kmeans_clustering(\n",
        "    lance_table,\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_clusters=n_clusters_based_on_analysis\n",
        ")\n",
        "print_clusters(clusters_kmeans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EbzXYaeF9_R"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clusters_to_summaries_table(\n",
        "    clusters,\n",
        "    client,\n",
        "    db_path\n",
        "):\n",
        "\n",
        "    summarizied_docs = []\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        summary = summarize_cluster(i, client, cluster)\n",
        "        summarizied_docs.append(\n",
        "            {\n",
        "                \"text\": summary,\n",
        "                \"children\": [node.index for node in cluster],\n",
        "                \"id\": i\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "    summaries_db_path = db_path\n",
        "    shutil.rmtree(summaries_db_path, ignore_errors=True)\n",
        "\n",
        "    summaries_db = lancedb.connect(summaries_db_path)\n",
        "\n",
        "    summaries_lance_table = summaries_db.create_table(\n",
        "        \"transformer_docs\",\n",
        "        mode='overwrite',\n",
        "        schema=BasicSchema\n",
        "    )\n",
        "\n",
        "    summaries_lance_table.add(\n",
        "        summarizied_docs,\n",
        "        on_bad_vectors='drop'\n",
        "    )\n",
        "    return summaries_lance_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv1t9EqOF9_R"
      },
      "outputs": [],
      "source": [
        "summaries_lance_table_cluster_analysis = clusters_to_summaries_table(\n",
        "    clusters=clusters_kmeans,\n",
        "    client=client,\n",
        "    db_path=\"/tmp/summaries_lancedb_cluster_analysis\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-72-12G9F9_R"
      },
      "source": [
        "We can now use these summaries to answer the original questions within the RAG pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiO24zW2F9_R"
      },
      "outputs": [],
      "source": [
        "answer = answer_with_rag(\n",
        "    QUESTION,\n",
        "    client=client,\n",
        "    model_name=ANSWERING_MODEL_NAME,\n",
        "    make_context=prepare_context_maker_raptor(lance_table, summaries_lance_table_cluster_analysis),\n",
        "    verbose=True,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
